{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3>Semester Project: Conclusions and Results of Heart Disease Prediction Analysis</h3>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction:**\n",
    "\n",
    ">In this semester-long project, I developed a logistic regression model to predict the likelihood of a heart attack based on various health attributes. This comprehensive analysis involved data preprocessing, feature selection, model training, and evaluation using the Cleveland heart disease dataset.\n",
    "\n",
    ">The initial exploration of the dataset aimed to understand its structure and identify any missing values or outliers. The dataset contained 303 records with 14 attributes each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns # We'll try using this, because it's very neat :D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('heart.csv')\n",
    "\n",
    "# Data info and stats\n",
    "print(data.info())\n",
    "print(data.describe())\n",
    "\n",
    "# Correlation heatmap: :) - Figure 1 fyi\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(data.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Distribution of target variable :> - Figure 2 btw\n",
    "sns.countplot(data['target'])\n",
    "plt.title('Distribution of Target Variable')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Selection:**\n",
    "\n",
    "> Using Recursive Feature Elimination (RFE), I selected the most relevant features for the logistic regression model. This step was crucial in reducing dimensionality and improving model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Select relevant features based on domain knowledge and correlation analysis :D\n",
    "features = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n",
    "target = 'target'\n",
    "\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# Handle categorical features using one-hot encoding\n",
    "X = pd.get_dummies(X, columns=['cp', 'restecg', 'slope', 'thal'], drop_first=True) # it goess brrrr\n",
    "\n",
    "# Scale numerical features\n",
    "scalar = StandardScaler()\n",
    "X_scaled = scalar.fit_transform(X)\n",
    "\n",
    "# FINALLY now we can do da feature selection :D\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "rfe = RFE(model, n_features_to_select=8)\n",
    "rfe.fit(X_scaled, y)\n",
    "selected_features = X.columns[rfe.support_]\n",
    "print(f\"Selected Features: {selected_features}\")\n",
    "\n",
    "# Selected X\n",
    "X_selected = X[selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Training and Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning (why do I do this to myself?)\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "grid_search = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train the logistic regression model with the best hyperparameters\n",
    "best_model = LogisticRegression(max_iter=1000, C=best_params['C'])\n",
    "best_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Evaluation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "# We boutta evaluate da model :D ^^\n",
    "\n",
    "# Make predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "auc_roc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# PRINT ALL THE THINGS\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Classification Report:\\n{class_report}\")\n",
    "print(f\"AUC-ROC: {auc_roc}\")\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_roc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
